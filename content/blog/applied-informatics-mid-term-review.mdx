---
title: "Applied Informatics期中复习"
date: "2023-05-29"
description: "我就这一门有期中考试，但是很烦"
image: /images/blog/pizza-girl.jpg
authors:
  - KUD
---

<Callout>
本文使用了katex,如遇到数学公式重复渲染等问题，可以刷新解决（刷新会重新加载解决此问题的js）
</Callout>

Standard NN
Recurrent NN(RNN)
Convolutional NN(CNN)

ML and DL difference: Feature extraction in ML is by human

---

# Linear Regression

$y=wx+b$
- x and y are given, inter(cept) and slope are unknown.

---

Least Squares for Simple Linear Regression
Loss function:
$$
L(w,b) = \frac{1}{m} \int^m_{i=1} (y[i] - (w x[i] + b))^2 \\
$$

---

$$
\frac{\partial L(w,b)}{\partial w} = 0 \\
$$

---

$$
\frac{\partial L(w,b)}{\partial b} = 0
$$

---

$$
w = \frac{\sum^m_{i=1} (x[i] - \bar{x}) (y[i] - \bar{y})}{\sum^m_{i=1} (x[i] - \bar{x})^2}
$$

---

<Callout>
这个w的结果算出来好像xy协方差比x的方差啊……
</Callout>

> 因为最小二乘法为简单线性回归问题提供了一种求解方法，其中 w 的计算结果可以被解释为 x 和 y 的协方差除以 x 的方差。具体来说，w 表示 x 和 y 之间的线性关系的斜率，而 x 和 y 的协方差则度量它们之间的相关性，除以 x 的方差可以将其标准化，从而得到一个无单位的值。因此，w 的计算结果与 x 和 y 的协方差比上 x 的方差非常相似。

<Callout>
又忘了最小二乘法是什么了……我是傻逼
</Callout>

---

Gradient Descent

$$
\frac{\partial L(w,b)}{\partial w} = \frac{\partial \frac{1}{m} \sum^m_{i=1}(y[i] - (wx[i] + b))^2}{\partial w} = \frac{-2}{m} \sum^m_{i=1} x[i](y[i] - \hat{y}) = \frac{-2}{m}\sum^m_{i=1}x[i](y[i] - (wx[i] + b))
$$

---
$$
w = w - \alpha \frac{\partial L(w, b)}{\partial w}
$$

---

$$
b = b - \alpha \frac{\partial L(w, b)}{\partial b} \\
$$

> 每次迭代中，梯度下降算法计算损失函数关于权重和偏置的偏导数（梯度），并朝着损失函数下降最快的方向更新权重和偏置。这是因为梯度给出了损失函数在当前点的最陡峭方向。因此，通过更新权重和偏置以沿着负梯度方向移动，我们可以最小化损失函数

<Callout>
道理看起来好像明白了，但是实际上如何，如何直观的理解，还是需要借助数学作图工具以后再看一眼
</Callout>

Can be used for linear model and other model

---

# NN
## Logistic Regression

Probability: $P (y = 1 | x), 0\leqslant P \leqslant 1$

odds: $P/(1-P)$, 0 to infinity

from linear regression to logistic regression:
- $ln(\frac{P}{1-P}) = wx + b$

$$
odds = \frac{P}{1-P} = e^{(wx+b)}
$$

---

$$
P = \frac{1}{(1+e^{(-(wx-b))})}
$$

> 数学上来说，逻辑回归可以看作是一种广义线性回归模型（Generalized Linear Model,GLM），它将线性回归模型的结果通过一个非线性函数（如Sigmoid函数）转换成概率值。

Sigmoid:

$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$

> 逻辑回归中，我们将线性组合通过sigmoid函数进行转换，可以得到一个在0到1之间的值，表示输入变量属于正类的概率

未知量仍然是w和b

---

## Logistic Regression Loss Function
