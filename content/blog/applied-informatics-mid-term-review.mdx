---
title: "Applied Informatics期中复习"
date: "2023-05-29"
description: "我就这一门有期中考试，但是很烦"
image: /images/blog/pizza-girl.jpg
authors:
  - KUD
---

<Callout>
本文使用了katex,如遇到数学公式重复渲染等问题，可以刷新解决（刷新会重新加载解决此问题的js）
</Callout>

Standard NN
Recurrent NN(RNN)
Convolutional NN(CNN)

ML and DL difference: Feature extraction in ML is by human

---

# Linear Regression

$y=wx+b$
- x and y are given, inter(cept) and slope are unknown.

---

Least Squares for Simple Linear Regression
Loss function:
$$
L(w,b) = \frac{1}{m} \int^m_{i=1} (y[i] - (w x[i] + b))^2 \\
$$

---

$$
\frac{\partial L(w,b)}{\partial w} = 0 \\
$$

---

$$
\frac{\partial L(w,b)}{\partial b} = 0
$$

---

$$
w = \frac{\sum^m_{i=1} (x[i] - \bar{x}) (y[i] - \bar{y})}{\sum^m_{i=1} (x[i] - \bar{x})^2}
$$

---

<Callout>
这个w的结果算出来好像xy协方差比x的方差啊……
</Callout>

> 因为最小二乘法为简单线性回归问题提供了一种求解方法，其中 w 的计算结果可以被解释为 x 和 y 的协方差除以 x 的方差。具体来说，w 表示 x 和 y 之间的线性关系的斜率，而 x 和 y 的协方差则度量它们之间的相关性，除以 x 的方差可以将其标准化，从而得到一个无单位的值。因此，w 的计算结果与 x 和 y 的协方差比上 x 的方差非常相似。

<Callout>
又忘了最小二乘法是什么了……我是傻逼
</Callout>

---

Gradient Descent

$$
\frac{\partial L(w,b)}{\partial w} = \frac{\partial \frac{1}{m} \sum^m_{i=1}(y[i] - (wx[i] + b))^2}{\partial w} = \frac{-2}{m} \sum^m_{i=1} x[i](y[i] - \hat{y}) = \frac{-2}{m}\sum^m_{i=1}x[i](y[i] - (wx[i] + b))
$$

---
$$
w = w - \alpha \frac{\partial L(w, b)}{\partial w}
$$

---

$$
b = b - \alpha \frac{\partial L(w, b)}{\partial b} \\
$$

> 每次迭代中，梯度下降算法计算损失函数关于权重和偏置的偏导数（梯度），并朝着损失函数下降最快的方向更新权重和偏置。这是因为梯度给出了损失函数在当前点的最陡峭方向。因此，通过更新权重和偏置以沿着负梯度方向移动，我们可以最小化损失函数

<Callout>
道理看起来好像明白了，但是实际上如何，如何直观的理解，还是需要借助数学作图工具以后再看一眼
</Callout>

Can be used for linear model and other model

---

# NN
## Logistic Regression

Probability: $P (y = 1 | x), 0\leqslant P \leqslant 1$

odds: $P/(1-P)$, 0 to infinity

from linear regression to logistic regression:
- $ln(\frac{P}{1-P}) = wx + b$

$$
odds = \frac{P}{1-P} = e^{(wx+b)}
$$

---

$$
P = \frac{1}{(1+e^{(-(wx-b))})}
$$

> 数学上来说，逻辑回归可以看作是一种广义线性回归模型（Generalized Linear Model,GLM），它将线性回归模型的结果通过一个非线性函数（如Sigmoid函数）转换成概率值。

Sigmoid:

$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$

导数是：

$$
\sigma '(z) = \sigma(z) (1 - \sigma (z))
$$

> 逻辑回归中，我们将线性组合通过sigmoid函数进行转换，可以得到一个在0到1之间的值，表示输入变量属于正类的概率

未知量仍然是w和b

---

## Logistic Regression Loss Function

$$
\hat{y}^{(i)} = \sigma (\theta^Tx^{(i)}), 0 \leqslant \hat{y}^{(i)} \leqslant 1, \sigma (z)  = \frac{1}{1 + e^{-z}}
$$

选择$\theta$让$\hat{y}^{(i)}$接近$y^{(i)}$

不太理解

---

这里还有些东西，求导数的部分 p25

---

浅NN也就是一堆感知机加在一起

这个training就不是很懂了，和导数挂钩的就很烦，求导都忘了 p26

后面还有一个backpropagation

前向传播（Forward Propagation）：
在前向传播过程中，输入数据通过网络的各个层，每一层都进行一系列的线性和非线性变换，最终得到网络的输出。这一过程也被称为网络的“前馈”过程。每个神经元接收来自上一层神经元的输入，将其加权求和并通过激活函数进行非线性映射。

反向传播（Backward Propagation）：
在反向传播过程中，首先计算损失函数对于网络输出的梯度。然后，通过应用链式法则（即导数的乘积规则），将梯度从输出层向输入层逐层反向传播。在每一层，根据链式法则，将上一层的梯度乘以当前层的权重和激活函数的导数，从而计算出当前层的梯度。最终，得到损失函数对于网络中所有参数的梯度。

通过计算梯度，我们可以使用梯度下降法或其他优化算法来更新网络的参数，以减小损失函数的值。反向传播通过反复迭代前向传播和梯度计算的过程，逐步优化网络参数，使得网络的预测结果更接近实际标签。

---

Week 4 先放弃

Week 5 没什么好看的

---

卷积：乘filter然后加起来

结果的形状是 输入的形状-卷积核的形状+1 (如果有padding要加上2p)

$$
\frac{n+2p-f}{s} + 1 (取下限)
$$

p32 notation

- Max Pooling: 和卷积差不多，就是挑最大的
- Average Pooling: 算平均值

p40 一个例子

---
