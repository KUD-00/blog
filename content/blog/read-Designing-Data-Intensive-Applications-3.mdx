---
title: "读Designing Data-Intensive Applications Chapter 3"
date: "2023-12-10"
description: "more!"
image: /images/blog/conbini-student.jpg
authors:
  - KUD
---

两类存储引擎：基于日志的存储引擎和基于页面的存储引擎，如B树

最简单的数据库：
```bash
db_set () {
    echo "$1,$2" >> database
}

db_get () {
    grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```

索引是从主数据派生出来的额外结构。许多数据库允许你添加和删除索引，这不会影响数据库的内容；它只会影响查询的性能。维护额外的结构会带来开销，尤其是在写入时。对于写入，简单地追加到文件的性能很难被超越，因为那是最简单的可能的写操作。任何类型的索引通常都会减慢写入速度，因为每次写入数据时也需要更新索引。

这是存储系统中的一个重要权衡：精心选择的索引可以加快读取查询的速度，但每个索引都会减慢写入速度。因此，数据库通常不会默认对所有内容进行索引，而是要求你——应用开发者或数据库管理员——手动选择索引，使用你对应用的典型查询模式的了解

假设我们的数据存储仅仅包括像前面的例子那样追加到文件中。那么最简单的索引策略就是这样：保持一个内存中的哈希映射，其中每个键都映射到数据文件中的一个字节偏移量——可以找到值的位置，如图3-1所示。每当你向文件追加一个新的键值对时，你也更新哈希映射以反映你刚刚写入的数据的偏移量（这适用于插入新键和更新现有键）。当你想查找一个值时，使用哈希映射找到数据文件中的偏移量，跳转到那个位置，并读取值。

这基本上就是 Bitcask（Riak 中默认的存储引擎）所做的。Bitcask 提供了高性能的读写，前提是所有键都适合于可用的 RAM，因为哈希映射完全保留在内存中。值可以使用比可用内存更多的空间，因为它们可以只通过一次磁盘寻道就从磁盘加载。如果数据文件的那部分已经在文件系统缓存中，读取根本不需要任何磁盘 I/O。

像 Bitcask 这样的存储引擎非常适合每个键的值经常更新的情况。例如，键可能是猫视频的 URL，值可能是它被播放的次数（每次有人点击播放按钮时递增）。在这种工作负载中，有很多写操作，但并不是太多不同的键——你有每个键大量的写操作，但将所有键保持在内存中是可行的

好的解决方案是通过关闭达到一定大小的段文件，并在新的段文件中进行后续写入，将日志分割成一定大小的段。然后，我们可以对这些段进行压缩，如图3-2所示。压缩意味着在日志中丢弃重复的键，并且只保留每个键的最新更新

段在写入后永远不会被修改，所以合并的段被写入新文件。合并和压缩段可以在后台线程中完成，同时进行时，我们仍然可以继续使用旧的段文件处理读请求，并将写请求写入最新的段文件。合并过程完成后，我们将读请求切换到使用新合并的段而不是旧段，并且旧的段文件可以简单地被删除

追加只读日志看起来很浪费：为什么不就地更新文件，用新值覆盖旧值呢？但追加只读设计出于几个原因是好的：
- 追加和段合并是顺序写操作，通常比随机写入快得多，尤其是在磁性旋转磁盘硬盘上。在一定程度上，顺序写入在基于闪存的固态硬盘（SSD）上也是可取的。我们将在“比较 B 树和 LSM 树”中进一步讨论这个问题
- 如果段文件是追加只读或不可变的，那么并发和崩溃恢复会简单得多。例如，你不必担心在值被覆盖时发生崩溃的情况，留下一个包含部分旧值和部分新值拼接在一起的文件。
- 合并旧段避免了数据文件随时间碎片化的问题。

哈希表索引也有其局限性：
- 哈希表必须适合内存，所以如果你有非常大量的键，就不太幸运了。原则上，你可以在磁盘上维护一个哈希映射，但不幸的是，很难让磁盘上的哈希映射表现良好。它需要大量的随机访问I/O，当它变满时扩容成本高昂，而且哈希冲突需要复杂的逻辑处理。
- 范围查询效率不高。例如，你不能轻易地扫描所有在kitty00000和kitty99999之间的键——你必须在哈希映射中单独查找每个键。

每个日志结构化存储段都是键值对的序列。这些对按照它们被写入的顺序出现，而日志中后面的值会覆盖日志中同一键的早期值。除此之外，文件中键值对的顺序并不重要。

现在我们可以对我们的段文件格式做一个简单的改变：我们要求键值对序列按键排序。我们将这种格式称为排序字符串表，或简称为SSTable。使用这种格式，我们不能立即将新的键值对追加到段中，因为写入可以按任何顺序发生；我们很快就会看到如何使用顺序I/O写入SSTable段。

SSTables与带有哈希索引的日志段相比有几个大优势：
- 合并段简单高效，即使文件大于可用内存。这种方法类似于归并排序算法中使用的方法，
- 为了在文件中找到特定的键，你不再需要在内存中保留所有键的索引。请参见图3-5的示例：假设你正在寻找键handiwork，但你不知道该键在段文件中的确切偏移量。然而，你确实知道键handbag和handsome的偏移量，由于排序，你知道handiwork必须出现在这两者之间。这意味着你可以跳转到handbag的偏移量，并从那里扫描，直到你找到handiwork（或者如果键不存在于文件中，则找不到）。
- 你仍然需要一个内存中的索引来告诉你某些键的偏移量，但它可以是稀疏的：每隔几千字节的段文件就有一个键就足够了，因为几千字节可以非常快速地扫描。

由于读取请求无论如何都需要扫描请求范围内的几个键值对，因此可以将这些记录分组成一个块，并在写入磁盘之前对其进行压缩（如图3-5中的阴影区域所示）。稀疏内存索引的每个条目都指向压缩块的起始处。除了节省磁盘空间外，压缩还减少了I/O带宽的使用

要构建和维护SSTable，一个重要的问题是，如何首先让数据按键排序？我们的写入操作可以按任何顺序发生。

在磁盘上维护排序结构是可能的（参见“B-树”），但在内存中维护它要容易得多。有很多众所周知的树数据结构可以使用，例如红黑树或AVL树。使用这些数据结构，你可以按任何顺序插入键，并按排序顺序读回它们。

我们现在可以使我们的存储引擎如下工作：
- 当写入请求到来时，将其添加到一个内存中的平衡树数据结构中（例如红黑树）。这个内存中的树有时被称为memtable。
- 当memtable大小超过某个阈值——通常是几兆字节——将其作为SSTable文件写入磁盘。这可以高效完成，因为树已经按键排序了键值对。新的SSTable文件成为数据库的最新段。在将SSTable写入磁盘时，写入可以继续到新的memtable实例。
- 为了处理读请求，首先尝试在memtable中找到键，然后在最新的磁盘段中找到键，然后在次新的段中找到键，依此类推。
- 不时地在后台运行合并和压缩过程，以合并段文件并丢弃被覆盖或删除的值。

这种方案运行得非常好。它只有一个问题：如果数据库崩溃，最近的写入（在memtable中但还没有写入磁盘）会丢失。为了避免这个问题，我们可以在磁盘上保持一个单独的日志，每个写入都立即追加到其中，就像前一节中所述。该日志不是排序的，但这不重要，因为它的唯一目的是在崩溃后恢复memtable。每次将memtable写入SSTable时，相应的日志可以被丢弃
